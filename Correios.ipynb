{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de Bancos: 21586\n",
      "Quantidade de Abreviacoes de Enderecos: 136\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CNPJ</th>\n",
       "      <th>SEQUENCIAL_DO_CNP</th>\n",
       "      <th>DV_DO_CNPJ</th>\n",
       "      <th>NOME_INSTITUICAO</th>\n",
       "      <th>SEGMENTO</th>\n",
       "      <th>COD_COMPE_AG</th>\n",
       "      <th>NOME_DA_AGENCIA</th>\n",
       "      <th>ENDERECO</th>\n",
       "      <th>NUMERO</th>\n",
       "      <th>COMPLEMENTO</th>\n",
       "      <th>BAIRRO</th>\n",
       "      <th>CEP</th>\n",
       "      <th>MUNICIPIO</th>\n",
       "      <th>UF</th>\n",
       "      <th>DATA_INICIO</th>\n",
       "      <th>DDD</th>\n",
       "      <th>FONE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21581</th>\n",
       "      <td>96.477.906</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>SOLIDEZ - CORRETORA DE CÂMBIO, TÍTULOS E VALOR...</td>\n",
       "      <td>Sociedade Corretora de TVM</td>\n",
       "      <td>0</td>\n",
       "      <td>MATRIZ</td>\n",
       "      <td>RUA XV DE NOVEMBRO 184  5 ANDAR CJ.501</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CENTRO</td>\n",
       "      <td>10139-040</td>\n",
       "      <td>SAO PAULO</td>\n",
       "      <td>SP</td>\n",
       "      <td>03/06/1993</td>\n",
       "      <td>11</td>\n",
       "      <td>32914788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21582</th>\n",
       "      <td>97.406.706</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>HP FINANCIAL SERVICES ARRENDAMENTO MERCANTIL S.A.</td>\n",
       "      <td>Sociedade de Arrendamento Mercantil</td>\n",
       "      <td>0</td>\n",
       "      <td>MATRIZ</td>\n",
       "      <td>ALAMEDA RIO NEGRO, Nº 750 - FUNDOS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2º ANDAR - SALA 3</td>\n",
       "      <td>ALPHAVILLE</td>\n",
       "      <td>06454-000</td>\n",
       "      <td>BARUERI</td>\n",
       "      <td>SP</td>\n",
       "      <td>23/01/2004</td>\n",
       "      <td>21</td>\n",
       "      <td>5321556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21583</th>\n",
       "      <td>97.406.706</td>\n",
       "      <td>3</td>\n",
       "      <td>51</td>\n",
       "      <td>HP FINANCIAL SERVICES ARRENDAMENTO MERCANTIL S.A.</td>\n",
       "      <td>Sociedade de Arrendamento Mercantil</td>\n",
       "      <td>0</td>\n",
       "      <td>RIO DE JANEIRO</td>\n",
       "      <td>RUA LAURO MULLER, 116</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SALA 802A - 8º ANDAR</td>\n",
       "      <td>BOTAFOGO</td>\n",
       "      <td>22290-160</td>\n",
       "      <td>RIO DE JANEIRO</td>\n",
       "      <td>RJ</td>\n",
       "      <td>06/11/2004</td>\n",
       "      <td>21</td>\n",
       "      <td>21426000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21584</th>\n",
       "      <td>97.406.706</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>HP FINANCIAL SERVICES ARRENDAMENTO MERCANTIL S.A.</td>\n",
       "      <td>Sociedade de Arrendamento Mercantil</td>\n",
       "      <td>0</td>\n",
       "      <td>PORTO ALEGRE</td>\n",
       "      <td>AVENIDA TAQUARA, 386</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SALA 704</td>\n",
       "      <td>PETROPOLIS</td>\n",
       "      <td>90460-210</td>\n",
       "      <td>PORTO ALEGRE</td>\n",
       "      <td>RS</td>\n",
       "      <td>13/11/2004</td>\n",
       "      <td>51</td>\n",
       "      <td>21213616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21585</th>\n",
       "      <td>97.447.304</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>MOMENTO DISTRIBUIDORA DE TITULOS E VALORES MOB...</td>\n",
       "      <td>Sociedade Distribuidora de TVM</td>\n",
       "      <td>0</td>\n",
       "      <td>MATRIZ</td>\n",
       "      <td>PRAIA DO FLAMENGO, 66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BLOCO A, LOJA 101 PARTE</td>\n",
       "      <td>FLAMENGO</td>\n",
       "      <td>22210-030</td>\n",
       "      <td>RIO DE JANEIRO</td>\n",
       "      <td>RJ</td>\n",
       "      <td>13/04/1994</td>\n",
       "      <td>21</td>\n",
       "      <td>25576411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CNPJ  SEQUENCIAL_DO_CNP  DV_DO_CNPJ  \\\n",
       "21581  96.477.906                  1          70   \n",
       "21582  97.406.706                  1          90   \n",
       "21583  97.406.706                  3          51   \n",
       "21584  97.406.706                  4          32   \n",
       "21585  97.447.304                  1          33   \n",
       "\n",
       "                                        NOME_INSTITUICAO  \\\n",
       "21581  SOLIDEZ - CORRETORA DE CÂMBIO, TÍTULOS E VALOR...   \n",
       "21582  HP FINANCIAL SERVICES ARRENDAMENTO MERCANTIL S.A.   \n",
       "21583  HP FINANCIAL SERVICES ARRENDAMENTO MERCANTIL S.A.   \n",
       "21584  HP FINANCIAL SERVICES ARRENDAMENTO MERCANTIL S.A.   \n",
       "21585  MOMENTO DISTRIBUIDORA DE TITULOS E VALORES MOB...   \n",
       "\n",
       "                                  SEGMENTO  COD_COMPE_AG NOME_DA_AGENCIA  \\\n",
       "21581           Sociedade Corretora de TVM             0          MATRIZ   \n",
       "21582  Sociedade de Arrendamento Mercantil             0          MATRIZ   \n",
       "21583  Sociedade de Arrendamento Mercantil             0  RIO DE JANEIRO   \n",
       "21584  Sociedade de Arrendamento Mercantil             0    PORTO ALEGRE   \n",
       "21585       Sociedade Distribuidora de TVM             0          MATRIZ   \n",
       "\n",
       "                                     ENDERECO NUMERO              COMPLEMENTO  \\\n",
       "21581  RUA XV DE NOVEMBRO 184  5 ANDAR CJ.501    NaN                      NaN   \n",
       "21582      ALAMEDA RIO NEGRO, Nº 750 - FUNDOS    NaN        2º ANDAR - SALA 3   \n",
       "21583                   RUA LAURO MULLER, 116    NaN     SALA 802A - 8º ANDAR   \n",
       "21584                    AVENIDA TAQUARA, 386    NaN                 SALA 704   \n",
       "21585                   PRAIA DO FLAMENGO, 66    NaN  BLOCO A, LOJA 101 PARTE   \n",
       "\n",
       "           BAIRRO        CEP       MUNICIPIO  UF DATA_INICIO  DDD      FONE  \n",
       "21581      CENTRO  10139-040       SAO PAULO  SP  03/06/1993   11  32914788  \n",
       "21582  ALPHAVILLE  06454-000         BARUERI  SP  23/01/2004   21   5321556  \n",
       "21583    BOTAFOGO  22290-160  RIO DE JANEIRO  RJ  06/11/2004   21  21426000  \n",
       "21584  PETROPOLIS  90460-210    PORTO ALEGRE  RS  13/11/2004   51  21213616  \n",
       "21585    FLAMENGO  22210-030  RIO DE JANEIRO  RJ  13/04/1994   21  25576411  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Agencias_Bancos= pd.read_csv(\"Dados/Agencias/Agencias.csv\",sep=\";\",encoding=\"latin1\")\n",
    "df_Abrev_Enderecos= pd.read_csv(\"Dados/Agencias/Abreviaturas_Enderco.csv\",sep=\",\",encoding=\"latin1\")\n",
    "Nome_Colunas=[\"CNPJ\",\"SEQUENCIAL_DO_CNP\",\"DV_DO_CNPJ\",\"NOME_INSTITUICAO\",\"SEGMENTO\",\"COD_COMPE_AG\",\"NOME_DA_AGENCIA\",\"ENDERECO\",\"NUMERO\",\"COMPLEMENTO\",\"BAIRRO\",\"CEP\",\"MUNICIPIO\",\"UF\",\"DATA_INICIO\",\"DDD\",\"FONE\"]\n",
    "df_Agencias_Bancos.columns=Nome_Colunas\n",
    "print (\"Quantidade de Bancos: \"+ str(len(df_Agencias_Bancos)))\n",
    "print (\"Quantidade de Abreviacoes de Enderecos: \"+ str(len(df_Abrev_Enderecos)))\n",
    "df_Agencias_Bancos.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer_portugues = nltk.stem.RSLPStemmer()\n",
    "def stem_sentences(sentence):\n",
    "    tokens = sentence.split()\n",
    "    stemmed_tokens = [stemmer_portugues.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "df_Agencias_Bancos.NOME_DA_AGENCIA=df_Agencias_Bancos.NOME_DA_AGENCIA.fillna(\"\")\n",
    "df_Agencias_Bancos['NOME_DA_AGENCIA_Steming'] = df_Agencias_Bancos['NOME_DA_AGENCIA'].apply(stem_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_CEP=[CEP_Text for CEP_Text in df_Agencias_Bancos.NOME_DA_AGENCIA_Steming]\n",
    "corpus_CPF=[CPF_Text for CPF_Text in df_Agencias_Bancos.NOME_DA_AGENCIA_Steming]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['manau', 'presid varg', 'sant', 'camp goytacaz', 'salv', 'recif', 's.publ fortal', 'prac tiradentes-pr', 'empr port alegr', 'empr joa pesso', 'tre coraco', 'macei', 'corumb', 'uberab', 'florianopolil', 'sao paul', 'ilh', 'sao luil', 'empr vitor', 'empr natal', 'parnaib', 'empr juiz de for', 'cataguas', 'carangol', 'jau', 'ribeira pret', 'empr pelot', 'pont gross', 'barret', 'varg', 'bel horizont', 'bag', 'sant livr', 'mossor', 'empr bauru', 'joinvill', 'camocim', 'sao felix', 'feir de sant', 'cacho do sul', 'ipamer', 'teresin', 'uruguai', 'empr cuiab', 'rio', 'afons pen', 'pened', 'catanduv', 'maca', 'empr campin', 'franc', 'beb', 'chav', 'piracicab', 'sao jos rio pret', 'lim', 'sant amar', 'jequi', 'teofil oton', 'barbacen', 'empr camp.grand', 'guaxup', 'sao joa boa vist', 'sao jos rio pard', 'garanhum', 'arcov', 'juaz', 'itabun', 'empr rio branc', 'nitero', 'barr do pir', 'itaperun', 'massarandub', 'taubat', 'piraju', 'pont por', 'botucatu', 'petropolil', 'nov iguacu', 'araraqu', 'cacho itapemirim', 'rio grand', 'sobr', 's.publ goian', 'catet', 'pont nov', 'cax do sul', 'araguar', 'madur', 'empr pass fund', 'bande', 'crat', 'empr blumenau', 'flori', 'presid prud', 'uberland', 'cajaz', 'jac', 'mei', 'port velh', 'curvel', 'mont clar', 'paraguacu paul', 'camp mai', 'cantagalo-rj', 'calcada', 'mund novo-b', 'vic', 'mirassol', 'colatin', 'sant anastaci', 'cafeland', 'palm', 'rio negr', 'propr', 'orland', 'nov horizont', 'unia do palm', 'aracat', 'iguatu', 'aquidau', 'cax', 'pirap', 'sant mar', 'camp grande-rj', 'caic', 'piripir', 'sant', 'resend', 'erechim', 'tup', 'mata', 'jacobin', 'palm do indi', 'joacab', 'sant angel', 'laje', 'foz do iguacu', 'maril', 'palm sol', 'canavi', 'alegret', 'mont aprazi', 'nov gran', 'jaguara', 'promissa', 'estanc', 'cab fri', 'pat', 'aracu', 'sao gabriel', 'ribeira bonit', 'bom jesu itabapo', 'ituver', 'alag', 'caruaru', 'piraju', 'andreland', 'sao joa del rei', 'pirassunung', 'itabaiana-pb', 'olimp', 'govern valad', 'braganc paul', 'alfen', 'quar', 'vac', 'itap', 'rio clar', 'boa esperanc', 'bic', 'sao jos do camp', 'camp belo-mg', 'carating', 'valparais', 'aracatub', 'sant cruz do sul', 'bent goncalv', 'irat', 'empr sa', 'cac', 'sao leopold', 'mim do sul', 'sao borj', 'vitor da conqu', 'pedern', 'pat de min', 'empr sorocab', 'camaqu', 'cruz alt', 'pass', 'dom pedrit', 'boa esperanc', 'canarana-mt', 'barir', 'itapetining', 'guarab', 'tubara', 'carl chag', 'av', 'ituiutab', 'our fin', 'mafr', 'aquidab', 'tre lago', 'sant terês', 'arax', 'maracaju', 'formig', 'pedr azul', 'assu', 'aim', 'lim', 'unia da vitor', 'sant cruz rio pard', 'burit alegr', 'goian', 'rio verd', 'sao mat', 'assil', 'corneli procopi', 'serr', 'pinh', 'barr', 'senh do bonfim', 'mont', 'caetit', 'barre', 'limo', 'vitor sant anta', 'cruz do sul-ac', 'sant vitor palm', 'presid olegari', 'crat', 'naz', 'sen pomp', 'amarg', 'quix', 'pedr', 'unia', 'sombri', 'ubaitab', 'serr talh', 'guirating', 'cod', 'ram', 'mont rora', 'piracuruc', 'braganc', 'pic', 'luziland', 'obid', 'taquariting', 'saude-rj', 'paranagu', 'macap', 'volt redond', 'tap', 'sant andr', 'sao cristova', 'dor do inda', 'bel vista-m', 'votuporang', 'jaboticab', 'uba', 'itaqu', 'ranch', 'andradin', 'patrocini', 'rio do sul', 'goi', 'itabaiana-s', 'lucel', 'capel', 'alegr', 'itambe-b', 'janu', 'almen', 'itaberab', 'muria', 'botafog', 'tijuc', 'copacab', 'garc', 'carolin', 'par de min', 'are', 'mog da cruz', 'sao carl', 'bangu', 'noss senh lap', 'penh franc', 'guarapu', 'empr pca arvor', 'ipiranga-sp', 'sao manuel', 'empr bra', 'rio pard', 'itaj', 'guaratinguet', 'lag', 'itajub', 'poc de cald', 'martinopolil', 'catala', 'sant antoni padu', 'jat', 'empr nov.hamburg', 'tre rio', 'manhuacu', 'camb', 'montenegr', 'americ', 'presid venceslau', 'chapec', 'sao caet do sul', 'russ', 'anapolil', 'dant barret', 'itacoati', 'sao lourenc do sul', 'pompe', 'duqu de cax', 'rosari do sul', 'sant do ipanem', 'ipu', 'parintim', 'baturit', 'nov friburg', 'lagart', 'tupanciret', 'arroi grand', 'sant ros', 'jundi', 'arar', 'guaib', 'can', 'diamantin', 'lagun', 'cidad alt', 'penapolil', 'birigu', 'roland', 'morr', 'batat', 'maring', 'santiag', 'itu', 'apucar', 'alem paraib', 'ipiau', 'car', 'arapong', 'mandaguar', 'curr nov', 'palm da misso', 'lago vermelh', 'lavr', 'coronel fabrici', 'mont carmel', 'farrap', 'pous alegr', 'sto.antoni patrulh', 'guacu', 'iju', 'divinopolil', 'dracen', 'itapipoc', 'cac', 'itumbi', 'form', 'raul so', 'our', 'paracatu', 'paranav', 'dianopolil', 'camp elise', 'mooc', 'pinh', 'empr sant', 'sant amar paul', 'ass', 'jequitinhonh', 'guajara-mirim', 'dour', 'cineland', 'bocaiuv', 'sao goncal', 'set lago', 'capel', 'guanha', 'tupacigu', 'mach', 'ur', 'brusqu', 'fernandopolil', 'vid', 'valenc', 'jaragu do sul', 'camp moura', 'cricium', 's.sebastia parais', 'nov prat', 'concord', 'jal', 'manhumirim', 'mococ', 'cruz da alm', 'atiba', 'taqu', 'itapeting', 'cas branc', 'igarap', 'itar', 'tre pont', 'frut', 'encant', 'encruzilh do sul', 'itaun', 'sto.antoni platin', 'sao bernard camp', 'vic', 'bandeir', 'estrel', 'guapor', 'guararap', 'juaz do nort', 'manten', 'prac mau', 'tup paul', 'sao luiz gonzag', 'francisc sa', 'osvald cruz', 'per barret', 'porecatu', 'luz', 'oliv', 'getuli varg', 'itajuip', 'timbaub', 'estrel do sul', 'mirandopolil', 'cruzeiro-sp', 'nhande', 'empr centr', 'sao fidelil', 'sacr', 'ampar', 'tre pass', 'cer', 'sta.barb d oest', 'angr do reil', 'sant dumont-mg', 'pires do rio', 'bom sucess', 'garibald', 'sao francisc do sul', 'itapolil', 'resplen', 'barr mans', 'adamantin', 'leopoldin', 'empr abolica', 'our pret', 'espirit sto.pinh', 'gua', 'astorg', 'macau', 'linh', 'cano', 'nanuqu', 'maranguap', 'corint', 'sao gotard', 'paranaib', 'castr', 'farroupilh', 'rio pomb', 'sao sep', 'sta.m do suacu', 'soledad', 'goiatub', 'registr', 'jacarepagu', 'sao francisc', 'pat branc', 'inhum', 'vicent de carvalh', 'pacaembu', 'sao jeron', 'sarand', 'carm do paranaib', 'empr cont', 'conselh lafaiet', 'ibiting', 'inhapim', 'paul de far', 'un', 'nov esperanc', 'itapev', 'tatu', 'alt aragua', 'porangatu', 'port ferr', 'palm de goi', 'cruz do oest', 'curitib', 'itapemirim', 'sao joa do piau', 'loand', 'pomb', 'bambu', 'sao roqu', 'espin', 'ipanem', 'quirinopolil', 'banan', 'bacab', 'uruacu', 's.luil mont bel', 'casca', 'ubaj', 'coraca de jesu', 'batalh', 'quixeramobim', 'sao bent do una', 'coromandel', 'ararangu', 'arra', 'arapirac', 'gui lop da lagun', 'piracanjub', 'valenc', 'caiapon', 'ico', 'irec', 'ilh do govern', 'bom conselh', 'rondonopolil', 'coxim', 'poxor', 'empr imperatriz', 'alenqu', 'poco', 'anicum', 'brev', 'itapurang', 'sao joa nepomucen', 'sao francisc assil', 'itapecuru-mirim', 'sant antoni jesu', 'ibicar', 'marab', 'pinh', 'altam', 'grajau', 'sant mar vitor', 'afog ingaz', 'barr do garc', 'goiand', 'lencoil paul', 'pindamonhangab', 'gram', 'empr penh', 'tef', 'mogi-mirim', 'caravel', 'mor sal', 'ori', 'surubim', 'tatuap', 'vil mar', 'catol do roch', 'xanx', 'toled', 'bom despach', 'bom jesu', 'brej', 'conceica m.dentr', 'vil milit', 'ir', 'remans', 'urucu', 'zon sul-rj', 'leblon', 'sao miguel do oest', 'araripin', 'coarac', 'ibait', 'sao joa do pat', 'veranopolil', 'cabrob', 'poss', 'conselh pen', 'sao jos do egit', 'corr', 'esplan', 'estanc velh', 'noss senh glor', 'sant ine', 'nov cruz', 'palm', 'francisc beltra', 'cangucu', 'cianort', 'juli de castilh', 'nov londrin', 'paul afons', 'tanab', 'mirand', 'cass', 'sap', 'candelaria-r', 'rio bonit', 'viama', 'timb', 'lap', 'mat verd', 'ipor', 'ivaip', 'pianc', 'cambuc', 'guarulh', 'osasc', 'araguain', 'juss', 'brej sant', 'gua', 'jaragu', 'tamoi', 'capinz', 'umuaram', 'ana ros', 'vil prud', 'itanhandu', 'prat', 'ipanem', 'ribeira do pinh', 'sapirang', 'muzamb', 'sao mat do sul', 'sao joaquim', 'cuit', 'jac', 'min', 'pass da are', 'pocon', 'ceagesp', 'bel vist parais', 'telemac borb', 'sant fe do sul', 'rosari oest', 'goianes', 'antoni pr', 'cacap do sul', 'taquar', 'venanci air', 'carpin', 'sao bent do sul', 'prac do correi', 'paranacity', 'ibirub', 'tap', 'cerr larg', 'freder westphalen', 'mau', 'tre de mai', 'jac', 'riacha do jacuip', 'camp nov', 'fregues do o', 'abaet', 'macaran', 'sant helen goi', 'campin verd', 'porte', 'castr alv', 'osori', 'camp larg', 'ibiram', 'set de setembr', 'canel', 'capivar', 'acopi', 'sant cruz', 'nov venec', 'wenceslau braz', 'pontalin', 'giru', 'aparec do tabo', 'fax do soturn', 'castanh', 'panamb', 'barr', 'sta.cruz capibarib', 'aven paul', 'tiet', 'cab', 'sao sebastiao-sp', 'alecrim', 'diadem', 'suzan', 'belen', 'bel jardim', 'moem', 'pinh mach', 'sao francisc paul', 'fl da cunh', 'marau', 'joa cam', 'nov andradin', 'itagu', 'brum', 'sananduv', 'sant august', 'camp sal', 'laranj do sul', 'median', 'palmit', 'tang', 'brac do nort', 'rio brilh', 'teresopolil', 'port murt', 'amamb', 'bom jesu da lap', 'cacequ', 'borrazopolil', 'ubirat', 'bom jesu', 'paraun', 'betim', 'sao joa de merit', 'set campin', 'rio verd m.gross', 'itaitub', 'camp bom', 'portao-curitib', 'sao miguel aragua', 'mombac', 'sous', 'afons claudi', 'estei', 'canudos-p', 'lem', 'itab', 'camb', 'nativ', 'marcelin ram', 'mandaguacu', 'tabating', 'tob barret', 'sao jos do cedr', 'sao lourenc oest', 'torr', 'mach', 'rubiatab', 'andr', 'barr do cord', 'camet', 'capinopolil', 'castelo-', 'cidad gauch', 'diamantin', 'elesba vel', 'engenh beltra', 'espum', 'euclid da cunh', 'eunapolil', 'glor de dour', 'gurup', 'horizontin', 'ipor', 'itaiopolil', 'itapirang', 'itatib', 'joa pinh', 'lavr do sul', 'montanha-', 'nilopolil', 'parais do tocantim', 'sto.antoni sudo', 'sao gabriel da palh', 'sao sebastia do cai', 'sobradinho-r', 'tai', 'tocantinopolil', 'val', 'vasso', 'brooklin paul', 'boc do acr', 'capita poc', 'carinhanh', 'ibotiram', 'manacapuru', 'manicor', 'paragomin', 'paul', 'port da folh', 'sant quit', 'sao miguel camp', 'ibiun', 'taguating centr', 'cubata', 'mogi-guacu', 'parais', 'alfandeg', 'asa nort 504', 'asa sul 507', 'empr r.assemble', 'empr goyaz', 'empr vil carra', 'bara de souz lea', 'bacacher', 'bara de duprat', 'carapicuib', 'caminh do mei', 'caju', 'carij', 'caxambu', 'empr s.campos-s', 'ilhot', 'merc s.sebastia', 'joa paul', 'liberdad', 'viscond', 'barrashopping', 'aven joa ces', 'noss sra.d lourd', 'vil form', 'par', 'perdizes-sp', 'pont pequen', 'prac silvi romer', 'rib', 'rudg ram', 'sao miguel paul', 'sao vicent', 'set de abril', 'sacoman', 'sobradinho-df', 'uting', 'varadour', 'aven cursin sp', 'vil alpin', 'vil div', 'vil galva', 'vil velh', 'vil belmir', 'acesit', 'tiriric', 'empr sia', 'gam centr', 'nucle bandeir', 'camp alegre-', 'ceasa-recif', 'aratu', 'liberdad', 'itambe-pr', 'parais do sul', 'presid varg', 'mediciland', 'cachoeirinha-r', 'ipu', 'distrit industr', 'bara de itap', 'denis', 'empr savass', 'itabirit', 'guanamb', 'bonito-p', 'salgu', 'boquim', 'princes isabel', 'mair', 'jeremoab', 'marac', 'icoarac', 'licini de alme', 'bonfim', 'imper', 'educ', 'av.fernand cunh', 'sao jos do belmont', 'una', 'camacar', 'mamanguap', 'sao paul do poteng', 'apod', 'jaboata guararap', 'conceica', 'sao jos do pinh', 'agu branca-p', 'bodoc', 'custod', 'mor nov', 'oro', 'empr av.saudad', 'gravat', 'pao de acuc', 'serta', 'janaub', 'humait', 'camp verd', 'conceica aragua', 'ipir', 'laj', 'silvan', 'flor paul', 'mairinqu', 'parqu da naco', 'fat do sul', 'gasp', 'seabr', 'salt', 'rib do pomb', 'umariz', 'jos bonifaci', 'lavr da mangab', 's.goncal do sapuc', 'sorocab nort', 'panel', 'doi vi', 'serr do caraj', 'bairr da pedr', 'boa viag', 'junqueiropolil', 'mont sant de min', 'palotin', 'piedad', 'rio mar', 'alv mach', 'camapu', 'cant do burit', 'luzian', 'pentecost', 'piratin', 'bom jesu de goi', 'ituram', 'mal.cand rondon', 'pitangu', 'port feliz', 'presid epitaci', 'janda do sul', 'palmit', 'port calv', 'ibi', 'votorantim', 'colin do tocantim', 'mutum', 'barr do bugr', 'atala', 'capanema-pr', 'inda', 'sao lourenc', 'jaci', 'sao domingos-g', 'terr ric', 'candelaria-rj', 'brasil de min', 'mont alt', 'tome-acu', 'xaxim-sc', 'salin', 'cord', 'guarapar', 'iun', 'agu form', 'arc', 'sum', 'piumh', 'pir', 'miracem', 'abaetetub', 'itanh', 'tre palm', 'tapiratib', 'mau', 'camp do jorda', 'barr bonita-sp', 'nono', 'cot', 'guadalup', 'trindad', 'campinort', 'mond', 'catuip', 'caarap', 'goio', 'itapaj', 'bezerr de menez', 'ipating', 'sao marc', 'rio da pedr', 'cacho paul', 'turv', 'trianon', 'chopin', 'sta.rit do sapuc', 'nao-me-toqu']\n",
      "['manau', 'presid varg']\n"
     ]
    }
   ],
   "source": [
    "corpus_CEP=corpus_CEP[:1000]\n",
    "print (corpus_CEP)\n",
    "corpus_CPF=corpus_CPF[:2]\n",
    "print (corpus_CPF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{0: 'manau'}, {0: 'manau'}],\n",
       " [{1: 'presid varg'}, {1: 'presid varg'}],\n",
       " [{1: 'presid varg'}, {860: 'presid varg'}]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "ti=time.time()\n",
    "incorrect = [[{Ind_CPF:Val_CPF},{Ind_CEP:Val_CEP}] for Ind_CPF,Val_CPF in enumerate(corpus_CPF)for Ind_CEP,Val_CEP in enumerate(corpus_CEP) if Val_CPF == corpus_CEP[Ind_CEP]]\n",
    "tf=time.time()\n",
    "print (tf-ti)\n",
    "incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict1={ind_CPF,[inds_CEPS]}\n",
    "dict_1={5:[201,203,204,205],25:[257,255],7:[345,346,347,348,349],17:[401,402,403],3:[702,708]}\n",
    "# Dictionario dict_Aux={ind_CPF:Numero}\n",
    "dict_Aux={5:422,25:333,7:500,17:1,3:55}\n",
    "# dict2={ind_CEP,[faixas_Numeros]}\n",
    "dict_2={2:['10','500'],3:['702','708'],201:['100','200'],203:['201','400'],204:['401','600'],205:['601','1000'],257:['10','600'],255:['601','1000'],345:['0','100'],346:['101','200'],347:['201','300'],348:['301','400'],349:['401','500'],401:['0','100'],402:['101','200'],403:['201','305'],5:['201','205'],25:['257','255'],100:['300','1000'],702:['1','999'],708:['0','1000'],17:['401','403']}\n",
    "# Converter a inteiro as listas\n",
    "dict_2_Integer = {keys:list(map(int, list_Values)) for keys,list_Values in dict_2.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{204: (401, 600),\n",
       " 257: (10, 600),\n",
       " 349: (401, 500),\n",
       " 401: (0, 100),\n",
       " 702: (1, 999),\n",
       " 708: (0, 1000)}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{inds_CEPS:(dict_2_Integer.get(inds_CEPS)[0],dict_2_Integer.get(inds_CEPS)[1]) for ind_CPF,Numero in dict_Aux.items() for inds_CEPS in dict_1.get(ind_CPF) if ((Numero>=dict_2_Integer.get(inds_CEPS)[0])&(Numero<=dict_2_Integer.get(inds_CEPS)[1]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de Acertos das Faixas de Numeros: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{5: 203, 25: 255, 7: 345, 17: 402, 3: 708}"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cazamento_Com_Faixa_Numeros={ind_CPF:inds_CEPS for ind_CPF,Numero in dict_Aux.items() for inds_CEPS in dict_1.get(ind_CPF) if ((Numero>=dict_2_Integer.get(inds_CEPS)[0])&(Numero<=dict_2_Integer.get(inds_CEPS)[1]))}\n",
    "print ('Quantidade de Acertos das Faixas de Numeros: '+ str(len(Cazamento_Com_Faixa_Numeros)))\n",
    "Cazamento_Com_Faixa_Numeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict1={ind_CPF,[inds_CEPS]}\n",
    "dict_1={5:[201,203],25:[257,255],7:[345,346],17:[401,402],3:[702,708]}\n",
    "# Dictionario dict_Aux={ind_CPF:Numero}\n",
    "dict_Aux={5:22,25:333,7:100,17:101,3:55}\n",
    "# dict2={ind_CEP,[faixas_Numeros]}\n",
    "dict_2={2:['10','500'],3:['702','708'],201:['0','200'],203:['1','199'],257:['0','600'],255:['1','599'],345:['0','100'],346:['1','99'],401:['100','700'],402:['101','699'],5:['201','205'],25:['257','255'],100:['300','1000'],702:['1','999'],708:['0','1000'],17:['401','403']}\n",
    "# Converter a inteiro as listas\n",
    "dict_2_Integer = {keys:list(map(int, list_Values)) for keys,list_Values in dict_2.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5: (0, 200), 7: (0, 100)}\n",
      "{25: (1, 599), 17: (101, 699), 3: (1, 999)}\n"
     ]
    }
   ],
   "source": [
    "# Com numeros Pares\n",
    "print ({ind_CPF:(dict_2_Integer.get(inds_CEPS)[0],dict_2_Integer.get(inds_CEPS)[1]) for ind_CPF,Numero in dict_Aux.items() if (Numero % 2==0) for inds_CEPS in dict_1.get(ind_CPF) if (dict_2_Integer.get(inds_CEPS)[0] %2==0)})\n",
    "# Com numeros Impares\n",
    "print ({ind_CPF:(dict_2_Integer.get(inds_CEPS)[0],dict_2_Integer.get(inds_CEPS)[1]) for ind_CPF,Numero in dict_Aux.items() if (Numero % 2==1) for inds_CEPS in dict_1.get(ind_CPF) if (dict_2_Integer.get(inds_CEPS)[0] %2==1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de Acertos com Numeros Pares: 2\n",
      "{5: 201, 7: 345}\n",
      "Quantidade de Acertos com Numeros Impares: 3\n",
      "{25: 255, 17: 402, 3: 702}\n"
     ]
    }
   ],
   "source": [
    "Cazamento_Com_Numeros_Pares={ind_CPF:inds_CEPS for ind_CPF,Numero in dict_Aux.items() if (Numero % 2==0) for inds_CEPS in dict_1.get(ind_CPF) if (dict_2_Integer.get(inds_CEPS)[0] %2==0)}\n",
    "print ('Quantidade de Acertos com Numeros Pares: '+ str(len(Cazamento_Com_Numeros_Pares)))\n",
    "print (Cazamento_Com_Numeros_Pares)\n",
    "Cazamento_Com_Numeros_Impares={ind_CPF:inds_CEPS for ind_CPF,Numero in dict_Aux.items() if (Numero % 2==1) for inds_CEPS in dict_1.get(ind_CPF) if (dict_2_Integer.get(inds_CEPS)[0] %2==1)}\n",
    "print ('Quantidade de Acertos com Numeros Impares: '+ str(len(Cazamento_Com_Numeros_Impares)))\n",
    "print (Cazamento_Com_Numeros_Impares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Com Bairro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict1={ind_CPF,[inds_CEPS]}\n",
    "dict_1={5:[201,203,204,205],25:[257,255],7:[345,346,347,348,349],17:[401,402,403],3:[702,708]}\n",
    "# Dictionario dict_Aux={ind_CPF:Numero}\n",
    "dict_Aux={5:'B',25:'F',7:'O',17:'R',3:'B'}\n",
    "# dict2={ind_CEP,[faixas_Numeros]}\n",
    "dict_2={2:'A',3:'B',201:'B',203:'B',204:'C',205:'D',257:'B',255:'F',345:'D',346:'L',347:'M',348:'N',349:'O',401:'R',402:'S',403:'T',5:'205',25:'A',100:'B',702:'A',708:'B',17:'R'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 201), (5, 203), (25, 255), (7, 349), (17, 401), (3, 708)]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(ind_CPF,inds_CEPS) for ind_CPF,Bairro in dict_Aux.items() for inds_CEPS in dict_1.get(ind_CPF) if (Bairro==dict_2.get(inds_CEPS))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting multiprocessing\n",
      "  Using cached https://files.pythonhosted.org/packages/b8/8a/38187040f36cec8f98968502992dca9b00cc5e88553e01884ba29cbe6aac/multiprocessing-2.6.2.1.tar.gz\n",
      "    Complete output from command python setup.py egg_info:\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\Users\\emily\\AppData\\Local\\Temp\\pip-install-fd15y1qu\\multiprocessing\\setup.py\", line 94\n",
      "        print 'Macros:'\n",
      "                      ^\n",
      "    SyntaxError: Missing parentheses in call to 'print'. Did you mean print('Macros:')?\n",
      "    \n",
      "    ----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Command \"python setup.py egg_info\" failed with error code 1 in C:\\Users\\emily\\AppData\\Local\\Temp\\pip-install-fd15y1qu\\multiprocessing\\\n",
      "You are using pip version 19.0.2, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "from joblib import Parallel, delayed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.570167303085327\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "ti=time.time()\n",
    "Parallel(n_jobs=2)(delayed(sqrt)(i ** 2) for i in range(100000))\n",
    "tf=time.time()\n",
    "print (tf-ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08101892471313477\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "ti=time.time()\n",
    "[sqrt(i ** 2) for i in range(100000)]\n",
    "tf=time.time()\n",
    "print (tf-ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math\n",
    "def eudis5(v1, v2):\n",
    "    dist = [(a - b)**2 for a, b in zip(v1, v2)]\n",
    "    dist = math.sqrt(sum(dist))\n",
    "    return dist\n",
    "def distancia_Euclideana(corpus):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    array=X.toarray()\n",
    "    v1=array[0]\n",
    "    v2=array[1]\n",
    "    distancia=eudis5(v1, v2)\n",
    "    if distancia==0.0:\n",
    "        print (vectorizer.get_feature_names())\n",
    "        return (distancia)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = ['This is the document.','This document is the second document.','This document is the second document.','This document is the second document asd.']\n",
    "# corpus_CPF = ['This is the document.','This document is the second document.','This document is the second document.','This document is the second document.']\n",
    "# distancia_Euclideana(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "ti=time.time()\n",
    "Comparacao = [[distancia_Euclideana([CEP_Text,CPF_Text]) for CEP_Text in corpus_CEP if (distancia_Euclideana([CEP_Text,CPF_Text])==0.0)] for CPF_Text in corpus_CPF]\n",
    "tf=time.time()\n",
    "print (tf-ti)\n",
    "Comparacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datalist=corpus_CEP\n",
    "filterList=corpus_CPF\n",
    "import time\n",
    "ti=time.time()\n",
    "CC=[CEP_Text for CEP_Text in corpus_CEP if any(CPF_Text for CPF_Text in corpus_CPF if CPF_Text == CEP_Text) ]\n",
    "tf=time.time()\n",
    "print (tf-ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(CC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist=[\"ab\",\"ac\",\"ad\",\"ad\",\"sf\"]\n",
    "filterList=[\"ab\",\"sf\",\"bc\",\"as\",\"bg\"]\n",
    "import time\n",
    "ti=time.time()\n",
    "C_Valores=[CEP_Text for CEP_Text in datalist if any(CPF_Text for CPF_Text in filterList if CPF_Text == CEP_Text) ]\n",
    "\n",
    "tf=time.time()\n",
    "print (tf-ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimina una linea de um arquivo\n",
    "def removeLine(filename, lineno):\n",
    "    fro = open(filename, \"rb\")\n",
    "\n",
    "    current_line = 0\n",
    "    while current_line < lineno:\n",
    "        fro.readline()\n",
    "        current_line += 1\n",
    "\n",
    "    seekpoint = fro.tell()\n",
    "    frw = open(filename, \"r+b\")\n",
    "    frw.seek(seekpoint, 0)\n",
    "\n",
    "    # read the line we want to discard\n",
    "    fro.readline()\n",
    "\n",
    "    # now move the rest of the lines in the file \n",
    "    # one line back \n",
    "    chars = fro.readline()\n",
    "    while chars:\n",
    "        frw.writelines(chars)\n",
    "        chars = fro.readline()\n",
    "\n",
    "    fro.close()\n",
    "    frw.truncate()\n",
    "    frw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from multiprocessing import Process\n",
    "import multiprocessing, time, signal\n",
    "import pickle\n",
    "import queue \n",
    "def mp_factorizer(nums, nprocs):\n",
    "    def worker(nums, out_q):\n",
    "        \"\"\" The worker function, invoked in a process. 'nums' is a\n",
    "            list of numbers to factor. The results are placed in\n",
    "            a dictionary that's pushed to a queue.\n",
    "        \"\"\"\n",
    "        outdict = {}\n",
    "        for n in nums:\n",
    "            outdict[n] = factorize_naive(n)\n",
    "        out_q.put(outdict)\n",
    "\n",
    "    # Each process will get 'chunksize' nums and a queue to put his out\n",
    "    # dict into\n",
    "    out_q = queue.Queue()\n",
    "    chunksize = int(math.ceil(len(nums) / float(nprocs)))\n",
    "    procs = []\n",
    "\n",
    "    for i in range(nprocs):\n",
    "        p = multiprocessing.Process(\n",
    "                target=worker,\n",
    "                args=(nums[chunksize * i:chunksize * (i + 1)],\n",
    "                      out_q))\n",
    "        procs.append(p)\n",
    "        p.start()\n",
    "\n",
    "    # Collect all results into a single result dict. We know how many dicts\n",
    "    # with results to expect.\n",
    "    resultdict = {}\n",
    "    for i in range(nprocs):\n",
    "        resultdict.update(out_q.get())\n",
    "\n",
    "    # Wait for all worker processes to finish\n",
    "    for p in procs:\n",
    "        p.join()\n",
    "\n",
    "    return resultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'mp_factorizer.<locals>.worker'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-6200ea15830c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmp_factorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-97-9a7288ff4821>\u001b[0m in \u001b[0;36mmp_factorizer\u001b[1;34m(nums, nprocs)\u001b[0m\n\u001b[0;32m     27\u001b[0m                       out_q))\n\u001b[0;32m     28\u001b[0m         \u001b[0mprocs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# Collect all results into a single result dict. We know how many dicts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't pickle local object 'mp_factorizer.<locals>.worker'"
     ]
    }
   ],
   "source": [
    "mp_factorizer([3,7,9,12], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queuelib import FifoDiskQueue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti=time.time()\n",
    "for i in range(10000):\n",
    "    print(i)\n",
    "for i in range(10000):\n",
    "    print(i*2)\n",
    "tf=time.time()\n",
    "print (tf-ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A loop\n",
    "def taskA():\n",
    "    for i in range(10000):\n",
    "        print(i)\n",
    "\n",
    "# B loop\n",
    "def taskB():\n",
    "    for i in range(10000):\n",
    "         print(i*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "ti=time.time()\n",
    "p1 = Process(target=taskA())\n",
    "p2 = Process(target=taskB())\n",
    "p1.start()\n",
    "p2.start()\n",
    "tf=time.time()\n",
    "print (tf-ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading \n",
    "ti=time.time()\n",
    "p1 = threading.Thread(target=taskA())\n",
    "p2 = threading.Thread(target=taskB())\n",
    "p1.start()\n",
    "p2.start()\n",
    "tf=time.time()\n",
    "print (tf-ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "pool = multiprocessing.Pool()\n",
    "toolbox.register(\"map\", pool.map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "pool = multiprocessing.Pool( args.numProcessors )\n",
    "tasks = []\n",
    "plotNum = 0\n",
    "while plotNum < args.numPlots:\n",
    "    plotNum += 1\n",
    "    tasks.append( (args.outputDir, plotNum, ) )\n",
    "results = [pool.apply_async( plotData, t ) for t in tasks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "any(i != j for i, j in zip(x_list, y_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AA=[print (Ind_CPF) for Ind_CPF,CPF_Text in enumerate(filterList, 0)]\n",
    "AA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AA=[{Ind_CEP,CEP_Text} for Ind_CEP,CEP_Text in enumerate(datalist, 0) if any(a=(Ind_CPF)(Ind_CPF,CPF_Text) for Ind_CPF,CPF_Text in enumerate(filterList, 0) if CPF_Text == CEP_Text) ]\n",
    "AA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[CPF_Text for CPF_Text in filterList if CPF_Text == CEP_Text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k,CEP_Text) for k, CEP_Text in enumerate(datalist, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_Valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.sean-lan.com/2016/09/12/jupyter-notebook-set-up/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padronizando os dados\n",
    "df_Abrev_Enderecos[\"Abrev\"]=df_Abrev_Enderecos[\"Abrev\"].str.lower()\n",
    "df_Abrev_Enderecos[\"Nome_Original\"]=df_Abrev_Enderecos[\"Nome_Original\"].str.lower()\n",
    "df_Agencias_Bancos[\"ENDERECO\"]=df_Agencias_Bancos[\"ENDERECO\"].str.lower()\n",
    "df_Agencias_Bancos[\"MUNICIPIO\"]=df_Agencias_Bancos[\"MUNICIPIO\"].str.lower()\n",
    "df_Agencias_Bancos[\"BAIRRO\"]=df_Agencias_Bancos[\"BAIRRO\"].str.lower()\n",
    "df_Agencias_Bancos[\"COMPLEMENTO\"]=df_Agencias_Bancos[\"COMPLEMENTO\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df_Agencias_Bancos.shape)\n",
    "df_Agencias_Bancos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Eliminando_Caracteres(col):\n",
    "    return col.str.replace(u'ç',u'c') \\\n",
    "            .str.replace(u'á|à|â|ã',u'a')\\\n",
    "            .str.replace(u'º|°',u'')\\\n",
    "            .str.replace(u'ó|ô|ó|õ',u'o') \\\n",
    "            .str.replace(u'ú|ù|û',u'u')\\\n",
    "            .str.replace(u'é|è|ê',u'e')\\\n",
    "            .str.replace(r'[0-9]',u' ',regex=True)\\\n",
    "            .str.replace(u'í|ì|î',u'i')\\\n",
    "            .str.replace(r'^[\" \"]+','', regex=True)\n",
    "%timeit Eliminando_Caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Agencias_Bancos['ENDERECO']=Eliminando_Caracteres(df_Agencias_Bancos['ENDERECO'])\n",
    "print (df_Agencias_Bancos.shape)\n",
    "df_Agencias_Bancos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_Agencias_Bancos.ENDERECO.str.split(\".\",0).str.get(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separando as primeiras letras da coluna do endereco\n",
    "#^\\S*\\s+\\K\\S*.+$\n",
    "# import re\n",
    "import regex as mrab\n",
    "df_Agencias_Bancos['Abrev_Endereco'] = df_Agencias_Bancos.ENDERECO.str.split(\".\",0).str.get(0)\n",
    "# \n",
    "bsk = mrab.compile(r'^\\S*\\s+\\K\\S*.+$')\n",
    "valor=bsk.search(df_Agencias_Bancos['Nom_Endereco'].values[0]).group()\n",
    "print(valor)\n",
    "# print(bsk.search(df_Agencias_Bancos['Abrev_Endereco'].values ))\n",
    "\n",
    "\n",
    "# df['EXTRACT'] = df.TEXT.str.extract('({})'.format('|'.join(word_list)), \n",
    "#                         flags=re.IGNORECASE, expand=False).str.lower().fillna('')\n",
    "\n",
    "df_Agencias_Bancos['EXTRACT'] = df_Agencias_Bancos['Nom_Endereco'].str.extract(valor, expand=True)\n",
    "# df_Agencias_Bancos['Nom_Endereco'] = df_Agencias_Bancos.ENDERECO.str.split(\".\",-1).str.get(1)\n",
    "df_Agencias_Bancos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t='d fdsfds f ds fd dsf sdfrt rgwsg ds'\n",
    "print(bsk.search(df_Agencias_Bancos['Nom_Endereco'].values[0]).group(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primeira Correcao (Inferencia Correta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Abrev_Enderecos[\"Abrev\"]=df_Abrev_Enderecos[\"Abrev\"].str.lower()\n",
    "df_Abrev_Enderecos[\"Nome_Original\"]=df_Abrev_Enderecos[\"Nome_Original\"].str.lower()\n",
    "df_Abrev_Enderecos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertendo to dictionary\n",
    "#Dic_Abrev_Enderecos=df_Abrev_Enderecos.set_index('Abrev').T.to_dict('split')\n",
    "Dic_Abrev_Enderecos=df_Abrev_Enderecos.set_index('Abrev')['Nome_Original'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Dic_Abrev_Enderecos.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcao de Correcao ortografica de acordo ao dicionario\n",
    "def f_Correcao_Abrev_to_Nome(df,Dic ,Coluna):\n",
    "    cont=0\n",
    "    for Valor_Abrev in df[Coluna]:\n",
    "        if Dic.get(Valor_Abrev)!=None:\n",
    "            df=df.set_value(cont,Coluna,Dic.get(Valor_Abrev))\n",
    "        cont=cont+1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Corregido_com_Pontos= f_Correcao_Abrev_to_Nome(df_Agencias_Bancos,Dic_Abrev_Enderecos,\"Abrev_Endereco\")\n",
    "df_Corregido_com_Pontos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Corregido_com_Pontos['as']= df_Corregido_com_Pontos.Abrev_Endereco.str.split(\" \").str.get(1)+\" \"+ df_Corregido_com_Pontos['BAIRRO'].values\n",
    "# df_Corregido_com_Pontos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Condicion que pega a primera letra que esta separado por espaco para ser remplazado na mesma columna\n",
    "Condicao_Sem_Ponto=(df_Corregido_com_Pontos.Abrev_Endereco.str.split(\" \").str.get(1).notnull())\n",
    "df_Corregido_com_Pontos['Abrev_2']=\"\"\n",
    "df_Corregido_com_Pontos.loc[Condicao_Sem_Ponto, 'Abrev_2']=df_Corregido_com_Pontos.Abrev_Endereco.str.split(\" \").str.get(1)\n",
    "df_Corregido_com_Pontos.loc[Condicao_Sem_Ponto, 'Abrev_Endereco']=df_Corregido_com_Pontos.Abrev_Endereco.str.split(\" \").str.get(0)\n",
    "df_Corregido_com_Pontos.Nom_Endereco=df_Corregido_com_Pontos.Nom_Endereco.fillna(\"\")\n",
    "df_Corregido_com_Pontos.loc[Condicao_Sem_Ponto, 'Nom_Endereco']=df_Corregido_com_Pontos.loc[Condicao_Sem_Ponto, 'Abrev_2']+df_Corregido_com_Pontos.Nom_Endereco\n",
    "df_Corregido_com_Pontos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Corregido_com_Pontos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Condicion que pega a primera letra que esta separado por espaco para ser remplazado na mesma columna\n",
    "Condicao_Sem_Ponto=(df_Corregido_com_Pontos.Abrev_Endereco.str.split(\",\").str.get(1).notnull())\n",
    "df_Corregido_com_Pontos.loc[Condicao_Sem_Ponto, 'Abrev_Endereco']=df_Corregido_com_Pontos.Abrev_Endereco.str.split(\",\").str.get(0)\n",
    "df_Corregido_com_Pontos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Condicion que pega a primera letra que esta separado por espaco para ser remplazado na mesma columna\n",
    "Condicao_Sem_Ponto=(df_Corregido_com_Pontos.Abrev_Endereco.str.split(\":\").str.get(1).notnull())\n",
    "df_Corregido_com_Pontos.loc[Condicao_Sem_Ponto, 'Abrev_Endereco']=df_Corregido_com_Pontos.Abrev_Endereco.str.split(\":\").str.get(0)\n",
    "df_Corregido_com_Pontos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Condicion que pega a primera letra que esta separado por espaco para ser remplazado na mesma columna\n",
    "Condicao_Sem_Ponto=(df_Corregido_com_Pontos.Abrev_Endereco.str.split(\";\").str.get(1).notnull())\n",
    "df_Corregido_com_Pontos.loc[Condicao_Sem_Ponto, 'Abrev_Endereco']=df_Corregido_com_Pontos.Abrev_Endereco.str.split(\";\").str.get(0)\n",
    "df_Corregido_com_Pontos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcao para Criar uma flag dos Corregidos\n",
    "def f_Coluna_Flag_to_Corregido(df,dic,Nome_Coluna_Compare,Nome_Coluna_Flag):\n",
    "    #Criando uma flag para ver que abreviacoes foram arrumadas\n",
    "    df[Nome_Coluna_Flag]=False\n",
    "    contagem=0\n",
    "    for Valor_Abrev in df[Nome_Coluna_Compare]:\n",
    "        if (Valor_Abrev in dic.values())==True:\n",
    "            df=df.set_value(contagem,Nome_Coluna_Flag,True)\n",
    "        contagem=contagem+1\n",
    "    return df, Nome_Coluna_Flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_Corregido_Ponto_Flag,Nome_Flag_Correc=f_Coluna_Flag_to_Corregido(df_Corregido_com_Pontos,Dic_Abrev_Enderecos,\"Abrev_Endereco\",\"Abrev_Ender_Flag_Correg\")\n",
    "print (\"Nome da Coluna Flag Criada: \"+Nome_Flag_Correc)\n",
    "#Determinando a Quantidade de ENderecos que faltam para arrumar\n",
    "Quantidade_Enderecos_Faltantes=len(df_Corregido_Ponto_Flag[df_Corregido_Ponto_Flag.Abrev_Ender_Flag_Correg==False])\n",
    "print (\"Quantidade de Enderecos para Tratar: \"+str(Quantidade_Enderecos_Faltantes))\n",
    "df_Corregido_Ponto_Flag.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segundo Tratamento (INFERENCIA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correcao Ortografica SEM PONTO\n",
    "Condicao_Sem_Ponto=(df_Corregido_Ponto_Flag.Abrev_Ender_Flag_Correg)\n",
    "df_Corregido_Ponto_Flag.loc[Condicao_Sem_Ponto==False, 'Abrev_Endereco']=f_Correcao_Abrev_to_Nome(df_Corregido_Ponto_Flag,Dic_Abrev_Enderecos,\"Abrev_Endereco\")\n",
    "df_Corregido_Ponto_Flag.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Agrupando aqueles que nao foram arrumados para o proximo Tratamento\n",
    "#df_Agencias_Bancos_Pra_Tratar=df_Corregido_Ponto_Flag[df_Corregido_Ponto_Flag[Nome_Flag_Correc]==False]\n",
    "#print (\"O numero de registros para arrumar sao: \"+str(len(df_Agencias_Bancos_Pra_Tratar)))\n",
    "#df_Agencias_Bancos_Pra_Tratar.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Atualiza o Flag com a Nova Inferencia\n",
    "df_Corregido_Flag_Inferencia,Nome_Flag_Correc_Inf=f_Coluna_Flag_to_Corregido(df_Corregido_Ponto_Flag,Dic_Abrev_Enderecos,\"Abrev_Endereco\",\"Abrev_Ender_Flag_Correg_Inferencia\")\n",
    "print (\"Nome da Coluna Flag Criada: \"+Nome_Flag_Correc_Inf)\n",
    "#Determinando a Quantidade de ENderecos que faltam para arrumar\n",
    "Qtd_Enderecos_Faltantes_Inferencia=len(df_Corregido_Flag_Inferencia[df_Corregido_Flag_Inferencia[Nome_Flag_Correc_Inf]==False])\n",
    "print (\"Quantidade de Enderecos para Tratar: \"+str(Qtd_Enderecos_Faltantes_Inferencia))\n",
    "df_Corregido_Flag_Inferencia.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INFERENCIA Drastica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Frecuencia_Abrev=df_Corregido_Flag_Inferencia.Abrev_Endereco.value_counts().to_frame()\n",
    "df_Frecuencia_Abrev=df_Frecuencia_Abrev.reset_index()\n",
    "df_Frecuencia_Abrev.columns=[\"Abrev_Nom\",\"Contagem\"]\n",
    "df_Frecuencia_Abrev.sort_values(\"Abrev_Nom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_Corregido_Flag_Inferencia[df_Corregido_Flag_Inferencia.Abrev_Endereco==\"avenida\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaaa.ENDERECO.values[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Avenida Albino de Almeida - lado par  \n",
    "Ananindeua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_Corregido_com_Pontos[[\"CNPJ\",\"COD_COMPE_AG\",\"ENDERECO\",\"Abrev_Endereco\",\"Abrev_Ender_Flag_Correg\"]].tail(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Corregido_Sem_Pontos\n",
    "df_Corregido_Sem_Pontos= f_Correcao_Abrev_to_Nome(df_Agencias_Bancos_Pra_Tratar,Dic_Abrev_Enderecos,\"Abrev_Endereco\")\n",
    "df_Corregido_Sem_Pontos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def ie_preprocess(document):\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    print (sentences)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    print (sentences)\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    print (sentences)\n",
    "\n",
    "ie_preprocess(df_Agencias_Bancos.ENDERECO[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_Agencias_Bancos[df_Agencias_Bancos.MUNICIPIO=='SAO PAULO'].CEP.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Agencias_Bancos.MUNICIPIO.unique()[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Agencias_Bancos.ENDERECO[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOME=\"AV.  PRES. VARGAS, 248\"\n",
    "ie_preprocess(NOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOME_2=\"PCA. DAS QUATRO JORNADAS em JOA CARLOS GUTERRES, 11\"\n",
    "ie_preprocess(NOME_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOME_2=\"PCA. DAS QUATRO JORNADAS em JR. JOA CARLOS GUTERRES, AV. PAULO SANTOS 11\"\n",
    "ie_preprocess(NOME_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear na linha de comando de anaconda um folder\n",
    "# scrapy startproject \"Nome_da_Pasta_do_Projeto_de_scrapy\"\n",
    "# entrar no endereço de esse projeto creado e abrir tudo no sublime\n",
    "# dentro da pasta de 'spyder' criar um arquivo com nome \"spider.py\"\n",
    "# Abrir o Arquivo \"items.py\"\n",
    "# e crear os items adecuados ao conjunto de dados que vc quer obter:\n",
    "# UF= scrapy.Field()\n",
    "# Localidade= scrapy.Field()\n",
    "# Tipo_Abrev= scrapy.Field()\n",
    "# Logradouro= scrapy.Field()\n",
    "# No_Lt= scrapy.Field()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
